# @package _group_

batch_size: 16
gradient_accumulation_steps: 1
num_train_epochs: 10
num_eval_per_epoch: 1

hard_negatives: 0
other_negatives: 0
shared_encoder: True # 试试
p_max_length: 150
q_max_length: 256
warmup_rate: 0 #0.1
warmup_steps: 0
train_insert_title: True
valid_insert_title: True

adam_eps: 1e-8
adam_betas: (0.9, 0.999)
learning_rate: 2e-5
max_grad_norm: 2.0
log_batch_step: 100
train_rolling_loss_step: 100
weight_decay: 0.0
val_av_rank_hard_neg: 30
val_av_rank_other_neg: 30
val_av_rank_bsz: 128
val_av_rank_max_qs: 10000
dev_batch_size: 16

sparse_warmup_rate: 0.1
sparse_warmup_flag: None
num_gates: 768
